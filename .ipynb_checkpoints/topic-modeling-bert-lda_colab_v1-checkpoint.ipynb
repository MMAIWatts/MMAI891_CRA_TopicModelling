{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"topic-modeling-bert-lda.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"_kg_hide-output":true,"id":"XaJoa7BIbwH1","colab_type":"code","outputId":"f962788b-8eba-4069-9ed6-7158ffc23bc5","executionInfo":{"status":"ok","timestamp":1590528734575,"user_tz":240,"elapsed":20279,"user":{"displayName":"Nafiseh Asghari","photoUrl":"","userId":"11961734450009038848"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# !pip install spacy-langdetect\n","# !pip install language-detector\n","# !pip install symspellpy\n","# !pip install sentence-transformers"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting spacy-langdetect\n","  Downloading https://files.pythonhosted.org/packages/29/70/72dad19abe81ca8e85ff951da170915211d42d705a001d7e353af349a704/spacy_langdetect-0.1.2-py3-none-any.whl\n","Collecting langdetect==1.0.7\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/59/59/4bc44158a767a6d66de18c4136c8aa90491d56cc951c10b74dd1e13213c9/langdetect-1.0.7.zip (998kB)\n","\u001b[K     |████████████████████████████████| 1.0MB 4.7MB/s \n","\u001b[?25hRequirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from spacy-langdetect) (3.6.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from langdetect==1.0.7->spacy-langdetect) (1.12.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->spacy-langdetect) (46.3.0)\n","Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->spacy-langdetect) (1.8.1)\n","Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->spacy-langdetect) (8.3.0)\n","Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->spacy-langdetect) (19.3.0)\n","Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->spacy-langdetect) (0.7.1)\n","Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->spacy-langdetect) (1.4.0)\n","Building wheels for collected packages: langdetect\n","  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for langdetect: filename=langdetect-1.0.7-cp36-none-any.whl size=993459 sha256=c243d6b41c9cd5769ada6d8dfaec31bfebaa2cf64ea8f5250bad0d8aef5a48d3\n","  Stored in directory: /root/.cache/pip/wheels/ec/0c/a9/1647275e7ef5014e7b83ff30105180e332867d65e7617ddafe\n","Successfully built langdetect\n","Installing collected packages: langdetect, spacy-langdetect\n","Successfully installed langdetect-1.0.7 spacy-langdetect-0.1.2\n","Collecting language-detector\n","  Downloading https://files.pythonhosted.org/packages/5c/0a/fefb61145a386968d2070323b608be6a1eb7508e610c2319daad746c2c33/language-detector-5.0.2.tar.gz\n","Building wheels for collected packages: language-detector\n","  Building wheel for language-detector (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for language-detector: filename=language_detector-5.0.2-cp36-none-any.whl size=7053 sha256=3a71714ba6f299c41dcffd13d50a34bc7c26db5061251b8fbe31f0fe4270b8de\n","  Stored in directory: /root/.cache/pip/wheels/7d/37/fa/2098a4aa6c0d94d6ddff0d3a79669e12bc4f7baca8a760b3db\n","Successfully built language-detector\n","Installing collected packages: language-detector\n","Successfully installed language-detector-5.0.2\n","Collecting symspellpy\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6d/0b/2daa14bf1ed649fff0d072b2e51ae98d8b45cae6cf8fdda41be01ce6c289/symspellpy-6.5.2-py3-none-any.whl (2.6MB)\n","\u001b[K     |████████████████████████████████| 2.6MB 4.6MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.13.1 in /usr/local/lib/python3.6/dist-packages (from symspellpy) (1.18.4)\n","Installing collected packages: symspellpy\n","Successfully installed symspellpy-6.5.2\n","Collecting sentence-transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b9/46/b7d6c37d92d1bd65319220beabe4df845434930e3f30e42d3cfaecb74dc4/sentence-transformers-0.2.6.1.tar.gz (55kB)\n","\u001b[K     |████████████████████████████████| 61kB 2.7MB/s \n","\u001b[?25hCollecting transformers>=2.8.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/12/b5/ac41e3e95205ebf53439e4dd087c58e9fd371fd8e3724f2b9b4cdb8282e5/transformers-2.10.0-py3-none-any.whl (660kB)\n","\u001b[K     |████████████████████████████████| 665kB 12.4MB/s \n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (4.41.1)\n","Requirement already satisfied: torch>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.5.0+cu101)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.18.4)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (0.22.2.post1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.4.1)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (3.2.5)\n","Collecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 46.8MB/s \n","\u001b[?25hCollecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 34.7MB/s \n","\u001b[?25hCollecting tokenizers==0.7.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n","\u001b[K     |████████████████████████████████| 3.8MB 43.0MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=2.8.0->sentence-transformers) (3.0.12)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.8.0->sentence-transformers) (2019.12.20)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers>=2.8.0->sentence-transformers) (2.23.0)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers>=2.8.0->sentence-transformers) (0.7)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.1->sentence-transformers) (0.16.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sentence-transformers) (0.15.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->sentence-transformers) (1.12.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.8.0->sentence-transformers) (7.1.2)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.8.0->sentence-transformers) (2020.4.5.1)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.8.0->sentence-transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.8.0->sentence-transformers) (2.9)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.8.0->sentence-transformers) (1.24.3)\n","Building wheels for collected packages: sentence-transformers, sacremoses\n","  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sentence-transformers: filename=sentence_transformers-0.2.6.1-cp36-none-any.whl size=74031 sha256=c7be00cf2d80a63ea42688b4ff9bfd182cb05f10c708ee272f2d1013d9335a24\n","  Stored in directory: /root/.cache/pip/wheels/d7/fa/17/2b081a8cd8b0a86753fb0e9826b3cc19f0207062c0b2da7008\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=ca1a0d661a05efc875e39908a03d2569a01d9e42d368838e4ef0fa114a35f1c2\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sentence-transformers sacremoses\n","Installing collected packages: sentencepiece, sacremoses, tokenizers, transformers, sentence-transformers\n","Successfully installed sacremoses-0.0.43 sentence-transformers-0.2.6.1 sentencepiece-0.1.91 tokenizers-0.7.0 transformers-2.10.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NUcFlk8awnrY","colab_type":"code","outputId":"7bd4ee96-006b-43fa-a673-b1bc3e310efb","executionInfo":{"status":"ok","timestamp":1590528661234,"user_tz":240,"elapsed":25315,"user":{"displayName":"Nafiseh Asghari","photoUrl":"","userId":"11961734450009038848"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rDpJUy0Hwvc4","colab_type":"code","outputId":"94446843-766d-4ffb-f905-57048fda5ccb","executionInfo":{"status":"ok","timestamp":1590528661237,"user_tz":240,"elapsed":25287,"user":{"displayName":"Nafiseh Asghari","photoUrl":"","userId":"11961734450009038848"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["%cd '/content/drive/My Drive/CRA_NLP'"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/CRA_NLP\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"HMeaDQrObwH7","colab_type":"text"},"source":["# Topic Modeling with BERT, LDA, and Clustering\n","#### Latent Dirichlet Allocation (LDA) probabilistic topic assignment and pre-trained sentence embeddings from BERT/RoBERTa"]},{"cell_type":"markdown","metadata":{"id":"iAw8kkW3bwH7","colab_type":"text"},"source":["**Insired by:** \n","\n","Shoa, S. (2020). **Contextual Topic Identification: Identifying meaningful topics for sparse Steam reviews**. Medium. Available at: https://blog.insightdatascience.com/contextual-topic-identification-4291d256a032 [Accessed 25 Mar. 2020]."]},{"cell_type":"markdown","metadata":{"id":"DbkqwIVibwH8","colab_type":"text"},"source":["## Model Deep Dive\n","\n","The author used: \n","\n","* LDA for probabilistic topic assignment vector.\n","* Bert for sentence embedding vector.\n","\n","1. Concatenated both LDA and Bert vectors with a weight hyperparameter to balance the relative importance of information from each source.\n","2. Used autoencoder to learn a lower dimensional latent space representation of the concatenated vector.\n","\n","* The assumption is that the concatendate vector shoul have a manifold shaep in the high dimensional space. \n","* USed clustering on the latent space representations to get topics. \n","\n"]},{"cell_type":"markdown","metadata":{"id":"xIVihavabwH9","colab_type":"text"},"source":["![Contextual Topic Identification model design](https://miro.medium.com/max/1410/1*OKCYnB-JbGq1NDwNSKd5Zw.png)"]},{"cell_type":"code","metadata":{"id":"HdTuNrRrbwH9","colab_type":"code","colab":{}},"source":["#importing all dependencies.\n","import os\n","import json\n","import pandas as pd\n","from tqdm import tqdm\n","import numpy as np\n","from nltk.corpus import wordnet\n","import re\n","import matplotlib.pyplot as plt\n","from nltk.corpus import stopwords \n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C8CUBKO0bwIB","colab_type":"text"},"source":["## Upload Data"]},{"cell_type":"markdown","metadata":{"id":"AddtiNS1bwIB","colab_type":"text"},"source":["**Data pipeline (from development to deployment**)\n","\n","![Data pipeline (from development to deployment)](https://miro.medium.com/max/1348/1*Cdp4y1tfMxqoj96o6lUdFg.png)\n","\n","\n","Source:Shoa "]},{"cell_type":"code","metadata":{"id":"UOlXw1s8bwIC","colab_type":"code","outputId":"0c0a9a99-3dd9-4f33-fa7f-18d0099a7e88","executionInfo":{"status":"ok","timestamp":1590528663431,"user_tz":240,"elapsed":27365,"user":{"displayName":"Nafiseh Asghari","photoUrl":"","userId":"11961734450009038848"}},"colab":{"base_uri":"https://localhost:8080/","height":71}},"source":["#load library\n","import os\n","import pandas as pd\n","import numpy as np\n","import gensim\n","from gensim.utils import simple_preprocess\n","from gensim import corpora, models\n","from gensim.parsing.preprocessing import STOPWORDS\n","from nltk.stem import WordNetLemmatizer, SnowballStemmer\n","from nltk.stem.porter import *\n","\n","import datetime\n","import time\n","from sklearn.decomposition import PCA\n","from sklearn.manifold import TSNE\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","from mpl_toolkits.mplot3d import Axes3D\n","\n","import nltk"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n","  import pandas.util.testing as tm\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"ShHJF1U3bwIF","colab_type":"code","outputId":"3d95e98a-8dfa-48ae-d2f9-e12768c81f20","executionInfo":{"status":"ok","timestamp":1590528665419,"user_tz":240,"elapsed":29272,"user":{"displayName":"Nafiseh Asghari","photoUrl":"","userId":"11961734450009038848"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["meta = pd.read_csv(\"data/MyA_TY2018_all.csv\")\n","print(meta.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (86) have mixed types.Specify dtype option on import or set low_memory=False.\n","  interactivity=interactivity, compiler=compiler, result=result)\n"],"name":"stderr"},{"output_type":"stream","text":["(66260, 107)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CEIY4I7KbwIO","colab_type":"code","outputId":"7f835163-56ae-4379-97e0-44b6b32311f3","executionInfo":{"status":"ok","timestamp":1590528665422,"user_tz":240,"elapsed":29252,"user":{"displayName":"Nafiseh Asghari","photoUrl":"","userId":"11961734450009038848"}},"colab":{"base_uri":"https://localhost:8080/","height":514}},"source":["#rename the column name\n","text_col = \"Please tell us why you were dissatisfied with today's experience. Please be as specific as possible.\"\n","rename_text_col = \"dissatisfaction_reason\"\n","\n","\n","meta.rename(columns= {text_col:rename_text_col}, inplace=True)\n","\n","#filter english only and not null rows\n","documents = meta.loc[(~meta['dissatisfaction_reason'].isna()) & (meta['Language answered'] == 'EN'),\n","                      ['dissatisfaction_reason']]\n","documents = documents.reset_index()\n","\n","\n","pd.options.display.max_colwidth = 200\n","documents.head(15)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>index</th>\n","      <th>dissatisfaction_reason</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>17</td>\n","      <td>couldn't find my gst assessment  it logged me out/timed out during setup/printed a blank page/says only ONE page to be printed, printed 3 pages  2 were BLANK with just URL</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>36</td>\n","      <td>more dissatisfied by the cra and the time table for my reassesment of my 2016 taxes</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>56</td>\n","      <td>This site is not user friendly at all, specially for people, who are not accustomed to the use of computer.</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>58</td>\n","      <td>The option to login on the canada.ca site feels hidden</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>72</td>\n","      <td>could not access my spouse (my wife account) yet I am her representative, We are living in Czech republic not easy to reach by the phone (expensive)</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>85</td>\n","      <td>TFSA contribution amount 2018 is not updated yet.</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>116</td>\n","      <td>Didn’t really get an answer</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>120</td>\n","      <td>No explanation as to when I will receive my refund from a reassessment of med exp &amp; no contact # to enquire</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>129</td>\n","      <td>I mailed my 2015 tax return 2.5 months ago and I am unable to confirm receipt nor find out it's processing status</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>139</td>\n","      <td>Je reçois une lettre qui me donne des renseignements qui ne semblent pas nécessaire parce que mon compte ouvre sans que les renseignements aient été captés!</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>149</td>\n","      <td>my account balance for taxes owing was unavailable</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>156</td>\n","      <td>The system was down so I was unable to proceed with the required banking changes that I wished to make and will have to wait 3 days.</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>169</td>\n","      <td>the system maintenance is going on could not get information</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>178</td>\n","      <td>Reports filed 9 months ago are missing</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>187</td>\n","      <td>The EI benefits  page didnt work</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    index                                                                                                                                                       dissatisfaction_reason\n","0      17  couldn't find my gst assessment  it logged me out/timed out during setup/printed a blank page/says only ONE page to be printed, printed 3 pages  2 were BLANK with just URL\n","1      36                                                                                          more dissatisfied by the cra and the time table for my reassesment of my 2016 taxes\n","2      56                                                                  This site is not user friendly at all, specially for people, who are not accustomed to the use of computer.\n","3      58                                                                                                                       The option to login on the canada.ca site feels hidden\n","4      72                         could not access my spouse (my wife account) yet I am her representative, We are living in Czech republic not easy to reach by the phone (expensive)\n","5      85                                                                                                                            TFSA contribution amount 2018 is not updated yet.\n","6     116                                                                                                                                                  Didn’t really get an answer\n","7     120                                                                  No explanation as to when I will receive my refund from a reassessment of med exp & no contact # to enquire\n","8     129                                                            I mailed my 2015 tax return 2.5 months ago and I am unable to confirm receipt nor find out it's processing status\n","9     139                 Je reçois une lettre qui me donne des renseignements qui ne semblent pas nécessaire parce que mon compte ouvre sans que les renseignements aient été captés!\n","10    149                                                                                                                           my account balance for taxes owing was unavailable\n","11    156                                         The system was down so I was unable to proceed with the required banking changes that I wished to make and will have to wait 3 days.\n","12    169                                                                                                                 the system maintenance is going on could not get information\n","13    178                                                                                                                                       Reports filed 9 months ago are missing\n","14    187                                                                                                                                             The EI benefits  page didnt work"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"f9JjFChUyLvF","colab_type":"code","outputId":"2458b70f-9815-4c3e-e3ff-564ca750d0f0","executionInfo":{"status":"ok","timestamp":1590528665690,"user_tz":240,"elapsed":29490,"user":{"displayName":"Nafiseh Asghari","photoUrl":"","userId":"11961734450009038848"}},"colab":{"base_uri":"https://localhost:8080/","height":514}},"source":["#drop french comment\n","documents.drop([9], axis = 0)\n","documents.head(15)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>index</th>\n","      <th>dissatisfaction_reason</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>17</td>\n","      <td>couldn't find my gst assessment  it logged me out/timed out during setup/printed a blank page/says only ONE page to be printed, printed 3 pages  2 were BLANK with just URL</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>36</td>\n","      <td>more dissatisfied by the cra and the time table for my reassesment of my 2016 taxes</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>56</td>\n","      <td>This site is not user friendly at all, specially for people, who are not accustomed to the use of computer.</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>58</td>\n","      <td>The option to login on the canada.ca site feels hidden</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>72</td>\n","      <td>could not access my spouse (my wife account) yet I am her representative, We are living in Czech republic not easy to reach by the phone (expensive)</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>85</td>\n","      <td>TFSA contribution amount 2018 is not updated yet.</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>116</td>\n","      <td>Didn’t really get an answer</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>120</td>\n","      <td>No explanation as to when I will receive my refund from a reassessment of med exp &amp; no contact # to enquire</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>129</td>\n","      <td>I mailed my 2015 tax return 2.5 months ago and I am unable to confirm receipt nor find out it's processing status</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>139</td>\n","      <td>Je reçois une lettre qui me donne des renseignements qui ne semblent pas nécessaire parce que mon compte ouvre sans que les renseignements aient été captés!</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>149</td>\n","      <td>my account balance for taxes owing was unavailable</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>156</td>\n","      <td>The system was down so I was unable to proceed with the required banking changes that I wished to make and will have to wait 3 days.</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>169</td>\n","      <td>the system maintenance is going on could not get information</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>178</td>\n","      <td>Reports filed 9 months ago are missing</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>187</td>\n","      <td>The EI benefits  page didnt work</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    index                                                                                                                                                       dissatisfaction_reason\n","0      17  couldn't find my gst assessment  it logged me out/timed out during setup/printed a blank page/says only ONE page to be printed, printed 3 pages  2 were BLANK with just URL\n","1      36                                                                                          more dissatisfied by the cra and the time table for my reassesment of my 2016 taxes\n","2      56                                                                  This site is not user friendly at all, specially for people, who are not accustomed to the use of computer.\n","3      58                                                                                                                       The option to login on the canada.ca site feels hidden\n","4      72                         could not access my spouse (my wife account) yet I am her representative, We are living in Czech republic not easy to reach by the phone (expensive)\n","5      85                                                                                                                            TFSA contribution amount 2018 is not updated yet.\n","6     116                                                                                                                                                  Didn’t really get an answer\n","7     120                                                                  No explanation as to when I will receive my refund from a reassessment of med exp & no contact # to enquire\n","8     129                                                            I mailed my 2015 tax return 2.5 months ago and I am unable to confirm receipt nor find out it's processing status\n","9     139                 Je reçois une lettre qui me donne des renseignements qui ne semblent pas nécessaire parce que mon compte ouvre sans que les renseignements aient été captés!\n","10    149                                                                                                                           my account balance for taxes owing was unavailable\n","11    156                                         The system was down so I was unable to proceed with the required banking changes that I wished to make and will have to wait 3 days.\n","12    169                                                                                                                 the system maintenance is going on could not get information\n","13    178                                                                                                                                       Reports filed 9 months ago are missing\n","14    187                                                                                                                                             The EI benefits  page didnt work"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"Q-ioB78rbwIR","colab_type":"text"},"source":["## Utils"]},{"cell_type":"code","metadata":{"id":"LeEjwAiF5Qf4","colab_type":"code","outputId":"42ca8ce0-c6dc-463d-9471-0927604eee27","executionInfo":{"status":"ok","timestamp":1590529253587,"user_tz":240,"elapsed":6220,"user":{"displayName":"Nafiseh Asghari","photoUrl":"","userId":"11961734450009038848"}},"colab":{"base_uri":"https://localhost:8080/","height":612}},"source":["!pip install pyLDAvis"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting pyLDAvis\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/3a/af82e070a8a96e13217c8f362f9a73e82d61ac8fff3a2561946a97f96266/pyLDAvis-2.1.2.tar.gz (1.6MB)\n","\u001b[K     |████████████████████████████████| 1.6MB 4.6MB/s \n","\u001b[?25hRequirement already satisfied: wheel>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.34.2)\n","Requirement already satisfied: numpy>=1.9.2 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (1.18.4)\n","Requirement already satisfied: scipy>=0.18.0 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (1.4.1)\n","Requirement already satisfied: pandas>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (1.0.3)\n","Requirement already satisfied: joblib>=0.8.4 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.15.1)\n","Requirement already satisfied: jinja2>=2.7.2 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (2.11.2)\n","Requirement already satisfied: numexpr in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (2.7.1)\n","Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (3.6.4)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.16.0)\n","Collecting funcy\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ce/4b/6ffa76544e46614123de31574ad95758c421aae391a1764921b8a81e1eae/funcy-1.14.tar.gz (548kB)\n","\u001b[K     |████████████████████████████████| 552kB 35.9MB/s \n","\u001b[?25hRequirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.17.0->pyLDAvis) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.17.0->pyLDAvis) (2.8.1)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.7.2->pyLDAvis) (1.1.1)\n","Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (8.3.0)\n","Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (1.8.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (46.3.0)\n","Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (19.3.0)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (1.12.0)\n","Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (1.4.0)\n","Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (0.7.1)\n","Building wheels for collected packages: pyLDAvis, funcy\n","  Building wheel for pyLDAvis (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyLDAvis: filename=pyLDAvis-2.1.2-py2.py3-none-any.whl size=97711 sha256=7979be9a3478d23134a04a27e50eab28709b4a4230c006ffc67b6a63105be8f4\n","  Stored in directory: /root/.cache/pip/wheels/98/71/24/513a99e58bb6b8465bae4d2d5e9dba8f0bef8179e3051ac414\n","  Building wheel for funcy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for funcy: filename=funcy-1.14-py2.py3-none-any.whl size=32042 sha256=35b3efb08aef29225b4b146da96bc20f8ef415b8060528a7441c27370ef88f4d\n","  Stored in directory: /root/.cache/pip/wheels/20/5a/d8/1d875df03deae6f178dfdf70238cca33f948ef8a6f5209f2eb\n","Successfully built pyLDAvis funcy\n","Installing collected packages: funcy, pyLDAvis\n","Successfully installed funcy-1.14 pyLDAvis-2.1.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UtUZ7sFfbwIR","colab_type":"code","outputId":"fc5e3387-3ead-4de4-9338-22c6d3483ca1","executionInfo":{"status":"ok","timestamp":1590528706995,"user_tz":240,"elapsed":4254,"user":{"displayName":"Nafiseh Asghari","photoUrl":"","userId":"11961734450009038848"}},"colab":{"base_uri":"https://localhost:8080/","height":170}},"source":["# !pip install umap-learn"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: umap-learn in /usr/local/lib/python3.6/dist-packages (0.4.3)\n","Requirement already satisfied: numba!=0.47,>=0.46 in /usr/local/lib/python3.6/dist-packages (from umap-learn) (0.48.0)\n","Requirement already satisfied: tbb in /usr/local/lib/python3.6/dist-packages (from umap-learn) (2020.0.133)\n","Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.6/dist-packages (from umap-learn) (0.22.2.post1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from umap-learn) (1.18.4)\n","Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.6/dist-packages (from umap-learn) (1.4.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from numba!=0.47,>=0.46->umap-learn) (46.3.0)\n","Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba!=0.47,>=0.46->umap-learn) (0.31.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20->umap-learn) (0.15.1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7c7GyuBAbwIV","colab_type":"code","colab":{}},"source":["from collections import Counter\n","from sklearn.metrics import silhouette_score\n","import umap\n","import matplotlib.pyplot as plt\n","from wordcloud import WordCloud\n","from gensim.models.coherencemodel import CoherenceModel\n","import numpy as np\n","import os\n","import pyLDAvis\n","import pyLDAvis.gensim  \n","\n","\n","def get_topic_words(token_lists, labels, k=None):\n","    \"\"\"\n","    get top words within each topic from clustering results\n","    \"\"\"\n","    if k is None:\n","        k = len(np.unique(labels))\n","    topics = ['' for _ in range(k)]\n","    for i, c in enumerate(token_lists):\n","        topics[labels[i]] += (' ' + ' '.join(c))\n","    word_counts = list(map(lambda x: Counter(x.split()).items(), topics))\n","    # get sorted word counts\n","    word_counts = list(map(lambda x: sorted(x, key=lambda x: x[1], reverse=True), word_counts))\n","    # get topics\n","    topics = list(map(lambda x: list(map(lambda x: x[0], x[:10])), word_counts))\n","\n","    return topics\n","\n","def get_coherence(model, token_lists, measure='c_v'):\n","    \"\"\"\n","    Get model coherence from gensim.models.coherencemodel\n","    :param model: Topic_Model object\n","    :param token_lists: token lists of docs\n","    :param topics: topics as top words\n","    :param measure: coherence metrics\n","    :return: coherence score\n","    \"\"\"\n","    if model.method == 'LDA':\n","        cm = CoherenceModel(model=model.ldamodel, texts=token_lists, corpus=model.corpus, dictionary=model.dictionary,\n","                            coherence=measure)\n","    else:\n","        topics = get_topic_words(token_lists, model.cluster_model.labels_)\n","        cm = CoherenceModel(topics=topics, texts=token_lists, corpus=model.corpus, dictionary=model.dictionary,\n","                            coherence=measure)\n","    return cm.get_coherence()\n","\n","def get_silhouette(model):\n","    \"\"\"\n","    Get silhouette score from model\n","    :param model: Topic_Model object\n","    :return: silhouette score\n","    \"\"\"\n","    if model.method == 'LDA':\n","        return\n","    lbs = model.cluster_model.labels_\n","    vec = model.vec[model.method]\n","    return silhouette_score(vec, lbs)\n","\n","def plot_proj(embedding, lbs):\n","    \"\"\"\n","    Plot UMAP embeddings\n","    :param embedding: UMAP (or other) embeddings\n","    :param lbs: labels\n","    \"\"\"\n","    n = len(embedding)\n","    counter = Counter(lbs)\n","    plt.figure(figsize = (15,15))\n","    for i in range(len(np.unique(lbs))):\n","        plt.plot(embedding[:, 0][lbs == i], embedding[:, 1][lbs == i], '.', alpha=0.5,\n","                 label='cluster {}: {:.2f}%'.format(i, counter[i] / n * 100))\n","    plt.legend(loc = 'best')\n","    plt.grid(color ='grey', linestyle='-',linewidth = 0.25)\n","\n","\n","\n","def visualize(model):\n","    \"\"\"\n","    Visualize the result for the topic model by 2D embedding (UMAP)\n","    :param model: Topic_Model object\n","    \"\"\"\n","    if model.method == 'LDA':\n","        return\n","    reducer = umap.UMAP()\n","    print('Calculating UMAP projection ...')\n","    vec_umap = reducer.fit_transform(model.vec[model.method])\n","    print('Calculating UMAP projection. Done!')\n","    plot_proj(vec_umap, model.cluster_model.labels_)\n","    dr = 'out/{}/{}'.format(model.method, model.id)\n","    if not os.path.exists(dr):\n","        os.makedirs(dr)\n","    plt.savefig('out/images/2D_vis')\n","\n","def get_wordcloud(model, token_lists, topic):\n","    \"\"\"\n","    Get word cloud of each topic from fitted model\n","    :param model: Topic_Model object\n","    :param sentences: preprocessed sentences from docs\n","    \"\"\"\n","    if model.method == 'LDA':\n","        return\n","    print('Getting wordcloud for topic {} ...'.format(topic))\n","    lbs = model.cluster_model.labels_\n","    tokens = ' '.join([' '.join(_) for _ in np.array(token_lists)[lbs == topic]])\n","\n","    wordcloud = WordCloud(width=800, height=560,\n","                          background_color='white', collocations=False,\n","                          min_font_size=10).generate(tokens)\n","\n","    # plot the WordCloud image\n","    plt.figure(figsize=(8, 5.6), facecolor=None)\n","    plt.imshow(wordcloud)\n","    plt.axis(\"off\")\n","    plt.tight_layout(pad=0)\n","    dr = 'out/{}/{}'.format(model.method, model.id)\n","    if not os.path.exists(dr):\n","        os.makedirs(dr)\n","    plt.savefig('out' + '/Topic' + str(topic) + '_wordcloud')\n","    print('Getting wordcloud for topic {}. Done!'.format(topic))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n7HLPJSqbwIX","colab_type":"text"},"source":["## Preprocessing "]},{"cell_type":"code","metadata":{"id":"TeWp55hQbwIY","colab_type":"code","outputId":"cf445f23-181f-42a4-ec68-7eb68691bea9","executionInfo":{"status":"ok","timestamp":1590528755251,"user_tz":240,"elapsed":5423,"user":{"displayName":"Nafiseh Asghari","photoUrl":"","userId":"11961734450009038848"}},"colab":{"base_uri":"https://localhost:8080/","height":170}},"source":["# !pip install stop-words"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting stop-words\n","  Downloading https://files.pythonhosted.org/packages/1c/cb/d58290804b7a4c5daa42abbbe2a93c477ae53e45541b1825e86f0dfaaf63/stop-words-2018.7.23.tar.gz\n","Building wheels for collected packages: stop-words\n","  Building wheel for stop-words (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for stop-words: filename=stop_words-2018.7.23-cp36-none-any.whl size=32917 sha256=dccd82740013424dcbe2498cbc1e6c36a645c79bc9342a418027a56008e10ede\n","  Stored in directory: /root/.cache/pip/wheels/75/37/6a/2b295e03bd07290f0da95c3adb9a74ba95fbc333aa8b0c7c78\n","Successfully built stop-words\n","Installing collected packages: stop-words\n","Successfully installed stop-words-2018.7.23\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BgoL7eIgbwIb","colab_type":"code","colab":{}},"source":["from stop_words import get_stop_words\n","from nltk.stem.porter import PorterStemmer \n","from nltk.stem import LancasterStemmer, WordNetLemmatizer\n","import re\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from language_detector import detect_language\n","\n","import pkg_resources\n","from symspellpy import SymSpell, Verbosity\n","\n","import string\n","import spacy\n","from spacy.lang.en import English\n","\n","sym_spell = SymSpell(max_dictionary_edit_distance=3, prefix_length=7)\n","dictionary_path = pkg_resources.resource_filename(\n","    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n","if sym_spell.word_count:\n","    pass\n","else:\n","    sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n","\n","\n","###################################\n","#### sentence level preprocess ####\n","###################################\n","\n","# lowercase + base filter\n","# some basic normalization\n","def f_base(s):\n","    \"\"\"\n","    :param s: string to be processed\n","    :return: processed string: see comments in the source code for more info\n","    \"\"\"\n","    # normalization 1: xxxThis is a --> xxx. This is a (missing delimiter)\n","    s = re.sub(r'([a-z])([A-Z])', r'\\1\\. \\2', s)  # before lower case\n","    # normalization 2: lower case\n","    s = s.lower()\n","    # normalization 3: \"&gt\", \"&lt\"\n","    s = re.sub(r'&gt|&lt', ' ', s)\n","    # normalization 4: letter repetition (if more than 2)\n","    s = re.sub(r'([a-z])\\1{2,}', r'\\1', s)\n","    # normalization 5: non-word repetition (if more than 1)\n","    s = re.sub(r'([\\W+])\\1{1,}', r'\\1', s)\n","    # normalization 6: string * as delimiter\n","    s = re.sub(r'\\*|\\W\\*|\\*\\W', '. ', s)\n","    # normalization 7: stuff in parenthesis, assumed to be less informal\n","    s = re.sub(r'\\(.*?\\)', '. ', s)\n","    # normalization 8: xxx[?!]. -- > xxx.\n","    s = re.sub(r'\\W+?\\.', '.', s)\n","    # normalization 9: [.?!] --> [.?!] xxx\n","    s = re.sub(r'(\\.|\\?|!)(\\w)', r'\\1 \\2', s)\n","    # normalization 10: ' ing ', noise text\n","    s = re.sub(r' ing ', ' ', s)\n","    # normalization 11: noise text\n","    s = re.sub(r'product received for free[.| ]', ' ', s)\n","    # normalization 12: phrase repetition\n","    s = re.sub(r'(.{2,}?)\\1{1,}', r'\\1', s)\n","\n","    return s.strip()\n","\n","\n","# language detection\n","def f_lan(s):\n","    \"\"\"\n","    :param s: string to be processed\n","    :return: boolean (s is English)\n","    \"\"\"\n","\n","    # some reviews are french\n","    return detect_language(s) in {'English', 'French'}\n","\n","\n","###############################\n","#### word level preprocess ####\n","###############################\n","\n","# filtering out punctuations and numbers\n","def f_punct(w_list):\n","    \"\"\"\n","    :param w_list: word list to be processed\n","    :return: w_list with punct and number filter out\n","    \"\"\"\n","    return [word for word in w_list if word.isalpha()]\n","\n","\n","# selecting nouns\n","def f_noun(w_list):\n","    \"\"\"\n","    :param w_list: word list to be processed\n","    :return: w_list with only nouns selected\n","    \"\"\"\n","    return [word for (word, pos) in nltk.pos_tag(w_list) if pos[:2] == 'NN']\n","\n","\n","# typo correction\n","def f_typo(w_list):\n","    \"\"\"\n","    :param w_list: word list to be processed\n","    :return: w_list with typo fixed by symspell. words with no match up will be dropped\n","    \"\"\"\n","    w_list_fixed = []\n","    for word in w_list:\n","        suggestions = sym_spell.lookup(word, Verbosity.CLOSEST, max_edit_distance=3)\n","        if suggestions:\n","            w_list_fixed.append(suggestions[0].term)\n","        else:\n","            pass\n","            # do word segmentation, deprecated for inefficiency\n","            # w_seg = sym_spell.word_segmentation(phrase=word)\n","            # w_list_fixed.extend(w_seg.corrected_string.split())\n","    return w_list_fixed\n","\n","\n","\n","\n","\n","#spacy\n","parser = English()\n","\n","#NLTK\n","# stemming if doing word-wise\n","# p_stemmer = PorterStemmer()\n","lemmer = WordNetLemmatizer()\n","\n","def f_stem(w_list):\n","    \"\"\"\n","    :param w_list: word list to be processed\n","    :return: w_list with stemming\n","    \"\"\"\n","    # sentence = ' '.join(w_list)\n","    # mytokens = parser(sentence)\n","    # return [word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens]\n","    return [lemmer.lemmatize(word) for word in w_list]\n","\n","    # retrun [p_stemmer.stem(word) for word in w_list]\n","\n","\n","# filtering out stop words\n","# create English stop words list\n","stop_words_to_keep = [\"no\", \"n't\", \"not\"]\n","stop_words = (list(\n","    set(get_stop_words('en'))\n","    |set(get_stop_words('fr'))\n","))\n","stop_words = [stop for stop in stop_words if stop not in stop_words_to_keep]\n","\n","\n","\n","def f_stopw(w_list):\n","    \"\"\"\n","    filtering out stop words\n","    \"\"\"\n","        \n","    #replace no and n't with not:\n","    x = [word for word in w_list if word not in stop_words]\n","    return ['not' if word in ['no', \"n't\"] else word for word in x]\n","\n","\n","def preprocess_sent(rw):\n","    \"\"\"\n","    Get sentence level preprocessed data from raw review texts\n","    :param rw: review to be processed\n","    :return: sentence level pre-processed review\n","    \"\"\"\n","    s = f_base(rw)\n","    if not f_lan(s):\n","        return None\n","    return s\n","\n","\n","def preprocess_word(s):\n","    \"\"\"\n","    Get word level preprocessed data from preprocessed sentences\n","    including: remove punctuation, select noun, fix typo, stem, stop_words\n","    :param s: sentence to be processed\n","    :return: word level pre-processed review\n","    \"\"\"\n","    if not s:\n","        return None\n","    w_list = word_tokenize(s)\n","    w_list = f_punct(w_list)\n","    w_list = f_noun(w_list)\n","    w_list = f_typo(w_list)\n","    w_list = f_stem(w_list)\n","    w_list = f_stopw(w_list)\n","\n","    return w_list\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Fr57kKzCbwIf","colab_type":"text"},"source":["## Autoencoder"]},{"cell_type":"code","metadata":{"id":"ne48d73NbwIf","colab_type":"code","outputId":"c6226153-074f-4825-88fa-4fadd0052367","executionInfo":{"status":"ok","timestamp":1590528766296,"user_tz":240,"elapsed":16417,"user":{"displayName":"Nafiseh Asghari","photoUrl":"","userId":"11961734450009038848"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import keras\n","from keras.layers import Input, Dense\n","from keras.models import Model\n","from sklearn.model_selection import train_test_split\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","\n","class Autoencoder:\n","    \"\"\"\n","    Autoencoder for learning latent space representation\n","    architecture simplified for only one hidden layer\n","    \"\"\"\n","\n","    def __init__(self, latent_dim=32, activation='relu', epochs=10, batch_size= 32):\n","        self.latent_dim = latent_dim\n","        self.activation = activation\n","        self.epochs = epochs\n","        self.batch_size = batch_size\n","        self.autoencoder = None\n","        self.encoder = None\n","        self.decoder = None\n","        self.his = None\n","\n","    def _compile(self, input_dim):\n","        \"\"\"\n","        compile the computational graph\n","        \"\"\"\n","        input_vec = Input(shape=(input_dim,))\n","        encoded = Dense(self.latent_dim, activation=self.activation)(input_vec)\n","        decoded = Dense(input_dim, activation=self.activation)(encoded)\n","        self.autoencoder = Model(input_vec, decoded)\n","        self.encoder = Model(input_vec, encoded)\n","        encoded_input = Input(shape=(self.latent_dim,))\n","        decoder_layer = self.autoencoder.layers[-1]\n","        self.decoder = Model(encoded_input, self.autoencoder.layers[-1](encoded_input))\n","        self.autoencoder.compile(optimizer='adam', loss=keras.losses.mean_squared_error)\n","\n","    def fit(self, X):\n","        if not self.autoencoder:\n","            self._compile(X.shape[1])\n","        X_train, X_test = train_test_split(X)\n","        self.his = self.autoencoder.fit(X_train, X_train,\n","                                        epochs=10,\n","                                        batch_size=32,\n","                                        shuffle=True,\n","                                        validation_data=(X_test, X_test), verbose=0)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"kiKQOEeNbwIi","colab_type":"text"},"source":["## Model"]},{"cell_type":"code","metadata":{"id":"4GDgS8U5bwIj","colab_type":"code","colab":{}},"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.cluster import KMeans\n","from gensim import corpora\n","import gensim\n","import numpy as np\n","#from Autoencoder import *\n","#from preprocess import *\n","from datetime import datetime\n","\n","\n","def preprocess(docs, samp_size=None):\n","    \"\"\"\n","    Preprocess the data\n","    \"\"\"\n","    if not samp_size:\n","        samp_size = 100\n","\n","    print('Preprocessing raw texts ...')\n","    n_docs = len(docs)\n","    sentences = []  # sentence level preprocessed\n","    token_lists = []  # word level preprocessed\n","    idx_in = []  # index of sample selected\n","    #     samp = list(range(100))\n","    samp = np.random.choice(n_docs, samp_size)\n","    for i, idx in enumerate(samp):\n","        sentence = preprocess_sent(docs[idx])\n","        token_list = preprocess_word(sentence)\n","        if token_list:\n","            idx_in.append(idx)\n","            sentences.append(sentence)\n","            token_lists.append(token_list)\n","        print('{} %'.format(str(np.round((i + 1) / len(samp) * 100, 2))), end='\\r')\n","    print('Preprocessing raw texts. Done!')\n","    return sentences, token_lists, idx_in\n","\n","\n","# define model object\n","class Topic_Model:\n","    def __init__(self, k=10, method='TFIDF'):\n","        \"\"\"\n","        :param k: number of topics\n","        :param method: method chosen for the topic model\n","        \"\"\"\n","        if method not in {'TFIDF', 'LDA', 'BERT', 'LDA_BERT'}:\n","            raise Exception('Invalid method!')\n","        self.k = k\n","        self.dictionary = None\n","        self.corpus = None\n","        #         self.stopwords = None\n","        self.cluster_model = None\n","        self.ldamodel = None\n","        self.vec = {}\n","        self.gamma = 15  # parameter for reletive importance of lda\n","        self.method = method\n","        self.AE = None\n","        self.id = method + '_' + datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n","\n","    def vectorize(self, sentences, token_lists, method=None):\n","        \"\"\"\n","        Get vecotr representations from selected methods\n","        \"\"\"\n","        # Default method\n","        if method is None:\n","            method = self.method\n","\n","        # turn tokenized documents into a id <-> term dictionary\n","        self.dictionary = corpora.Dictionary(token_lists)\n","        # convert tokenized documents into a document-term matrix\n","        self.corpus = [self.dictionary.doc2bow(text) for text in token_lists]\n","\n","        if method == 'TFIDF':\n","            print('Getting vector representations for TF-IDF ...')\n","            tfidf = TfidfVectorizer()\n","            vec = tfidf.fit_transform(sentences)\n","            print('Getting vector representations for TF-IDF. Done!')\n","            return vec\n","\n","        elif method == 'LDA':\n","            print('Getting vector representations for LDA ...')\n","            if not self.ldamodel:\n","                self.ldamodel = gensim.models.ldamodel.LdaModel(self.corpus, num_topics=self.k, id2word=self.dictionary,\n","                                                                passes=20)\n","\n","            def get_vec_lda(model, corpus, k):\n","                \"\"\"\n","                Get the LDA vector representation (probabilistic topic assignments for all documents)\n","                :return: vec_lda with dimension: (n_doc * n_topic)\n","                \"\"\"\n","                n_doc = len(corpus)\n","                vec_lda = np.zeros((n_doc, k))\n","                for i in range(n_doc):\n","                    # get the distribution for the i-th document in corpus\n","                    for topic, prob in model.get_document_topics(corpus[i]):\n","                        vec_lda[i, topic] = prob\n","\n","                return vec_lda\n","\n","            vec = get_vec_lda(self.ldamodel, self.corpus, self.k)\n","            print('Getting vector representations for LDA. Done!')\n","            return vec\n","\n","        elif method == 'BERT':\n","\n","            print('Getting vector representations for BERT ...')\n","            from sentence_transformers import SentenceTransformer\n","            model = SentenceTransformer('bert-base-nli-max-tokens')\n","            vec = np.array(model.encode(sentences, show_progress_bar=True))\n","            print('Getting vector representations for BERT. Done!')\n","            return vec\n","\n","             \n","        elif method == 'LDA_BERT':\n","        #else:\n","            vec_lda = self.vectorize(sentences, token_lists, method='LDA')\n","            vec_bert = self.vectorize(sentences, token_lists, method='BERT')\n","            vec_ldabert = np.c_[vec_lda * self.gamma, vec_bert]\n","            self.vec['LDA_BERT_FULL'] = vec_ldabert\n","            if not self.AE:\n","                self.AE = Autoencoder()\n","                print('Fitting Autoencoder ...')\n","                self.AE.fit(vec_ldabert)\n","                print('Fitting Autoencoder Done!')\n","            vec = self.AE.encoder.predict(vec_ldabert)\n","            return vec\n","\n","    def fit(self, sentences, token_lists, method=None, m_clustering=None):\n","        \"\"\"\n","        Fit the topic model for selected method given the preprocessed data\n","        :docs: list of documents, each doc is preprocessed as tokens\n","        :return:\n","        \"\"\"\n","        # Default method\n","        if method is None:\n","            method = self.method\n","        # Default clustering method\n","        if m_clustering is None:\n","            m_clustering = KMeans\n","\n","        # turn tokenized documents into a id <-> term dictionary\n","        if not self.dictionary:\n","            self.dictionary = corpora.Dictionary(token_lists)\n","            # convert tokenized documents into a document-term matrix\n","            self.corpus = [self.dictionary.doc2bow(text) for text in token_lists]\n","\n","        ####################################################\n","        #### Getting ldamodel or vector representations ####\n","        ####################################################\n","\n","        if method == 'LDA':\n","            if not self.ldamodel:\n","                print('Fitting LDA ...')\n","                self.ldamodel = gensim.models.ldamodel.LdaModel(self.corpus, num_topics=self.k, id2word=self.dictionary,\n","                                                                passes=20)\n","                print('Fitting LDA Done!')\n","        else:\n","            print('Clustering embeddings ...')\n","            self.cluster_model = m_clustering(self.k)\n","            self.vec[method] = self.vectorize(sentences, token_lists, method)\n","            self.cluster_model.fit(self.vec[method])\n","            print('Clustering embeddings. Done!')\n","\n","    def predict(self, sentences, token_lists, out_of_sample=None):\n","        \"\"\"\n","        Predict topics for new_documents\n","        \"\"\"\n","        # Default as False\n","        out_of_sample = out_of_sample is not None\n","\n","        if out_of_sample:\n","            corpus = [self.dictionary.doc2bow(text) for text in token_lists]\n","            if self.method != 'LDA':\n","                vec = self.vectorize(sentences, token_lists)\n","                print(vec)\n","        else:\n","            corpus = self.corpus\n","            vec = self.vec.get(self.method, None)\n","\n","        if self.method == \"LDA\":\n","            lbs = np.array(list(map(lambda x: sorted(self.ldamodel.get_document_topics(x),\n","                                                     key=lambda x: x[1], reverse=True)[0][0],\n","                                    corpus)))\n","        else:\n","            lbs = self.cluster_model.predict(vec)\n","        return lbs"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"59suh7H6bwIl","colab_type":"text"},"source":["## Training"]},{"cell_type":"code","metadata":{"id":"Qli_2UnvbwIm","colab_type":"code","colab":{}},"source":["#from model import *\n","#from utils import *\n","\n","import pandas as pd\n","import pickle\n","import matplotlib.pyplot as plt\n","\n","import warnings\n","warnings.filterwarnings('ignore', category=Warning)\n","\n","import argparse\n","\n","#def model(): #:if __name__ == '__main__':\n","\n","def main(method = \"LDA_BERT\"):\n","    \n","    \n","    # method = \"LDA_BERT\"\n","    samp_size = 4169\n","    ntopic = 9\n","    \n","    #parser = argparse.ArgumentParser(description='contextual_topic_identification tm_test:1.0')\n","\n","    #parser.add_argument('--fpath', default='data/train.csv')\n","    #parser.add_argument('--ntopic', default=10,)\n","    #parser.add_argument('--method', default='TFIDF')\n","    #parser.add_argument('--samp_size', default=20500)\n","    \n","    #args = parser.parse_args()\n","\n","    data = documents #pd.read_csv('data/train.csv')\n","    data = data.fillna('')  # only the comments has NaN's\n","    rws = data.dissatisfaction_reason\n","    sentences, token_lists, idx_in = preprocess(rws, samp_size=samp_size)\n","    # Define the topic model object\n","    #tm = Topic_Model(k = 10), method = TFIDF)\n","    tm = Topic_Model(k = ntopic, method = method)\n","    # Fit the topic model by chosen method\n","    tm.fit(sentences, token_lists)\n","    # Evaluate using metrics\n","    with open(\"out/{}.file\".format(tm.id), \"wb\") as f:\n","        pickle.dump(tm, f, pickle.HIGHEST_PROTOCOL)\n","\n","    print('Coherence:', get_coherence(tm, token_lists, 'c_v'))\n","    print('Silhouette Score:', get_silhouette(tm))\n","    # visualize and save img\n","    visualize(tm)\n","    for i in range(tm.k):\n","        get_wordcloud(tm, token_lists, i)\n","        \n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"izlVFXE1e1Ba","colab_type":"code","outputId":"03b3bc9a-faae-4144-81b7-a6e80d2a5164","executionInfo":{"status":"ok","timestamp":1590528768181,"user_tz":240,"elapsed":18144,"user":{"displayName":"Nafiseh Asghari","photoUrl":"","userId":"11961734450009038848"}},"colab":{"base_uri":"https://localhost:8080/","height":153}},"source":["import nltk\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('wordnet')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"_e6-6YIWbwIt","colab_type":"code","outputId":"3cd8d680-c7b0-496d-f81a-7ef872d558fd","executionInfo":{"status":"ok","timestamp":1590530005961,"user_tz":240,"elapsed":509045,"user":{"displayName":"Nafiseh Asghari","photoUrl":"","userId":"11961734450009038848"}},"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1_OkhQQXJnBTyp9MIUVv29neudFLOV6UF"}},"source":["main('LDA_BERT')"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"v5B2iaECzPpw","colab_type":"code","outputId":"9e6802ab-8c36-4852-97f0-4e3ded04a36b","executionInfo":{"status":"ok","timestamp":1590530603939,"user_tz":240,"elapsed":509699,"user":{"displayName":"Nafiseh Asghari","photoUrl":"","userId":"11961734450009038848"}},"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1R0Vo8J9eOWokhkLv7BeACuAuj32OddmN"}},"source":["main(\"BERT\")"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"nAN7x2BH1rLp","colab_type":"code","colab":{}},"source":["main('TFIDF')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"r9lKIfKXKFR2","colab_type":"code","colab":{}},"source":["main(\"LDA\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cIHNxr9YKFHF","colab_type":"code","colab":{}},"source":["main(\"TFIDF\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1hhsk4GzXWd2","colab_type":"text"},"source":["https://www.machinelearningplus.com/nlp/topic-modeling-visualization-how-to-present-results-lda-models/"]}]}